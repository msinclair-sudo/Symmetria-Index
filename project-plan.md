Thanks for confirming. I’ll generate a technical report assessing the current state of your nutrition project and lay out a concrete plan for building a local Python-based ML model to handle full end-to-end meal planning—from interpreting user profiles and goals to generating a validated 7-day meal plan.

I’ll let you know once the report is ready.


# Technical Evaluation and Development Plan for the Meal Planning System

## Data Completeness and Consistency Review

**Profile Data (`profile.json`):** The profile file is largely complete, containing personal info (age, sex, height, weight, activity level), daily and weekly nutritional targets (macros, fiber, and numerous micronutrients), and the allowed number of weekly cooking sessions. All essential sections (`DailyNutritionalNeeds`, `WeeklyNutritionalNeeds`, etc.) are present. One minor inconsistency is the key name for cooking sessions – the blueprint expects `"cook_sessions_per_week"`, but the profile uses `"cook_session_per_week"` (singular). This is a trivial naming issue, yet for strict validation it should be made consistent. The value is `"3"`, matching the requirement of exactly 3 weekly cooking sessions. The profile’s nutritional targets appear reasonable and comprehensive, covering calories, macros, fiber, and many micronutrients (vitamins, minerals, fatty acids, etc.). No obvious gaps are present in the profile data aside from the naming discrepancy.

**Goals Data (`goals.json`):** The goals file defines multiple goal presets (HighEnergy, LowEnergyFilling, ImmuneBoost, GutHealth, BudgetTimeSaver, HeartHealthy, BoneSupport, etc.), each with a `priority_nutrients` weight vector and `priority_tags` weights, plus a description. There are more than the minimum four goals required, which is good. All nutrient and tag weights fall between –1.0 and +1.0 as expected. However, **the weights are represented as numeric values (floats) rather than strings**. The blueprint specifies that all weights **MUST** be numeric *strings* in the JSON, but in the current `goals.json` they are unquoted numbers (e.g. `1.0` instead of `"1.0"`). This discrepancy could be a source of strict validation failure in an automated check. Otherwise, the goal definitions seem consistent and well-chosen, each emphasizing or de-emphasizing certain nutrients and tags to guide the meal plan optimization. No major content gaps are evident in goals; each has a clear description and a logical set of weighted priorities.

**Recipes Data (`recipes.json`):** The recipes file serves as the menu catalogue, containing 108 recipe entries with their details. Each recipe entry includes essential fields: Category (meal type), Cuisine, Servings, PrepTime, CookTime, Ingredients list, Instructions, a NutritionSummary of key macros, and Tags. Additionally, most recipes include fields like `"max leftover days"` and `"mealtime category"`, which are meant to guide scheduling (leftover shelf-life and whether the recipe is suited for breakfast, lunch, dinner, etc.). The presence of these fields indicates the data is geared toward enforcing rule C6 (leftover limit) and categorizing meals by time of day. The **NutritionSummary** for each recipe appears to list Calories, Protein, Carbohydrates, Fat, and Fiber – the major macronutrients. Micronutrients (vitamins and minerals) are **not listed per recipe**, which means the current data does not directly provide vitamin/mineral content for evaluating those targets. Also, **cost data is missing**: the blueprint expects a `cost_per_serving` value for each recipe (computed from ingredient costs), and also uses cost in scoring and constraints (penalizing recipes over \$5 per serving). Currently, recipes have no cost per serving field, and the ingredient lists do not include cost information either. This is a notable gap since budget considerations are part of the goals (e.g., “Low-Cost” tags, BudgetTimeSaver goal) and rule C5 enforces a cost limit.

Despite the missing cost and micronutrient details, the recipe entries are otherwise consistent. Ingredient names within recipes are fairly standardized (e.g. “Black Beans” vs “Chickpeas”), and should match an ingredients database. Each recipe’s Ingredients list uses consistent units and naming (cup, tablespoon, grams, etc.), but we must verify against an actual ingredients master list for consistency. **However, the `ingredients.json` master file is not provided in the expected format.** In fact, it appears the content of `ingredients.json` is identical to `recipes.json` (both files contain the recipe catalogue). There is **no separate ingredient nutrition/price table** in the given data, even though the blueprint mandates that *“Every ingredient referenced by any recipe MUST exist \[in ingredients.json]”* along with its nutrition and price info. This absence means we currently lack a structured source for each ingredient’s nutritional values (especially micronutrients) and cost per unit. The consistency between recipe ingredient names cannot be verified without that master list, although a quick scan of unique ingredient names (about 32 unique ingredients total) shows no obvious naming conflicts or duplicates. It will be necessary to create or obtain an ingredient database to fill this gap.

In summary, the JSON data covers user profile, goals, and a comprehensive recipe catalogue. The main completeness issues are the **missing ingredients nutrition/cost table** and **lack of cost and micronutrient details** in recipes. There are also minor consistency issues (key naming and data type mismatches relative to the blueprint specification). These need to be addressed before a fully compliant meal plan system can be built.

## Gaps and Potential Improvements in Data

Based on the review above, several gaps and improvement opportunities are identified in the current data structure and content:

* **Missing Ingredient Database:** The project lacks an `ingredients.json` with per-ingredient nutritional values and prices. This is crucial for computing each recipe’s nutrition and cost. **Improvement:** Create a structured ingredients file listing each ingredient (matching the names used in recipes) along with its macro and micronutrient content per unit (e.g. per 100g or per serving) and cost per unit. This will enable automated nutrition calculations and cost estimates for any recipe.

* **Cost Per Serving not populated:** No `cost_per_serving` field is present in recipes, even though the blueprint requires it for scoring and constraints. **Improvement:** Once ingredient prices are available, calculate cost per serving for each recipe (sum of ingredient costs divided by number of servings) and add this to the recipe data. This will allow enforcing the budget rule (max \$5 AUD per serving).

* **Micronutrient Data not in Recipes:** Recipes only list calories, macros, and fiber. Micronutrients like vitamins and minerals are not specified per recipe, yet the profile has targets for Vitamin A, C, D, etc., and the plan evaluation phase includes checks for key micronutrients (Vitamin C, Iron, Calcium, Magnesium, Zinc ≥ 90% targets). **Improvement:** Use the ingredient nutrition data to calculate a more complete nutrition profile for each recipe (or for each day’s meals on the fly). At minimum, ensure the system can compute those key micronutrients for each meal when building the plan, so that it can verify the constraints in Phase E.

* **Inconsistent JSON Formatting:** The data should consistently represent numeric values as strings if following the blueprint strictly. Currently, goal weights are numbers (not strings), and keys like `"max leftover days"` contain spaces instead of a unified format (`"max_leftover_days"` as suggested). **Improvement:** Standardize the JSON format – e.g., use underscores for multi-word keys, and decide on a convention for numeric fields (the blueprint suggests keeping them in quotes in JSON to avoid rounding or format issues). This will make parsing more reliable and align with the specification.

* **Dietary Constraints & Preferences:** The profile’s `dietary_notes` is a free-text field (e.g. “no dietary sensitivities; goal is weight maintenance”). Important preferences or restrictions (vegan, allergies, etc.) are not codified in a structured way. **Improvement:** If needed, introduce structured fields for dietary constraints (e.g., a list of forbidden ingredients or preferred diet type). This would make it easier for the system to automatically filter out recipes that violate those constraints.

* **Recipe Tagging Consistency:** The recipe tags cover diet traits (Vegetarian, Vegan, Gluten-Free), nutrition highlights (High-Protein, High-Fiber, etc.), and other descriptors (Quick, Low-Cost, etc.). They seem well-assigned, but it’s worth ensuring consistency (e.g., using “Low-Cost” vs “Budget-Friendly” – both appear in tags). **Improvement:** Standardize tags (possibly map synonyms like “Budget-Friendly” and “Low-Cost” to a single tag) to avoid diluting the tag-based scoring. Consistent tags will improve the accuracy of goal matching (since goals reference specific tags like `"High-Fiber"` or `"Low-Sodium"`).

* **Completeness of Recipe NutritionSummary:** The `nutrition_summary` field for recipes **should be complete** as per the blueprint. In practice, it covers the basics but omits micronutrients. While adding every micronutrient may be tedious, at least the key ones mentioned in profile targets could be added. Alternatively, the system can compute these on the fly, as long as the ingredient data is available. **Improvement:** Ensure that if a recipe’s NutritionSummary is incomplete, the system can recalculate and **inject** the missing values before using the data. This might be done at runtime in Phase A.

Addressing the above gaps will greatly improve the data quality and ensure the planning system has all the information needed to generate and validate meal plans.

## Design of an Offline Meal Planning Pipeline (Phases A–G)

To meet the project requirements, we propose a local Python-based system that implements the entire meal-planning workflow (phases A through G of the blueprint) without needing internet access. The system will take the user’s profile and goals as input (from JSON files) and produce a 7-day meal plan as JSON output. The design follows the blueprint’s phases:

* **Phase A – Data Loading & Validation:** The system will first load all JSON files (profile, goals, recipes, ingredients) and perform integrity checks. This includes verifying that every ingredient used in `recipes.json` has an entry in the ingredients database, checking for required fields and correct data types, and ensuring values are within expected ranges. For example, it will confirm that keys like `DailyNutritionalNeeds` and `WeeklyNutritionalNeeds` exist and that `cook_sessions_per_week` equals "3". It will also validate that goal weight values are between -1 and 1, recipe ingredient names match exactly with the ingredients list, and so on. Any failures here would prompt an error (as per the blueprint’s specified JSON error output). On success, the validated data moves to the next phase.

* **Phase B – Weekly Goal Selection:** In an interactive setting, the system would list available goals and let the user choose one. For a fully automated local tool, the goal can be provided as a parameter (or the user can be prompted in a console interface). The system will take the chosen goal (by its key, e.g., `"HighEnergy"` or `"WeightLoss"`) and retrieve the corresponding weight vector. It will then record this selection in the plan data (e.g., set `"goal_used"` and use the associated `priority_nutrients` and `priority_tags` as the active weights). If an invalid goal key is given, the system will produce an error as specified.

* **Phase C – Recipe Scoring:** Using the chosen goal’s weights, the system will score all recipes in the catalogue. The blueprint provides a formula that must be used for consistency. This formula combines nutrient achievement and tag matching:

  $$\text{score} = \sum_{i}\Big(\frac{\text{recipe\_nutrient}_i}{\text{daily\_target}_i}\Big) \times \text{goal\_weight}_i \;+\; 0.5\times(\#\text{matching tags}) \;-\; 0.5\times(\#\text{conflicting tags}) \;-\; 1\text{ if cost>\$5}$$

  In practice, the system will: (a) normalize each recipe’s nutrient values by the user’s daily needs (from the profile) for those nutrients that have a weight; (b) multiply by the weight; (c) sum across all nutrients. Then add 0.5 for each tag that the goal positively prioritizes which the recipe has, and subtract 0.5 for each tag that the goal negatively prioritizes that the recipe has. If the recipe’s cost per serving exceeds \$5 AUD, subtract a further 1 point penalty. This yields a numerical score for each recipe. The system will likely use a pandas DataFrame or numpy arrays for efficient vectorized computation of these scores across all recipes. It will then rank recipes by score and (if needed) present the top N. In a fully automated scenario, we might automatically take the top-ranked recipes forward. If interactive, the user could pin or swap recipes at this stage, but since the requirement is offline automation, we will assume the top-scoring recipes will feed into the plan generation.

* **Phase D – Menu Generation and Scheduling:** This is the core of the system – constructing a 7-day meal plan (with 3 meals per day) using the recipes and abiding by all cooking session and leftover rules. The system must schedule exactly 3 cooking sessions (by default on Sun, Tue, Thu or other user-chosen “user-friendly” days). Each cooking session will produce 2–3 recipes worth of food. The meals for the week are then composed of fresh servings on the cooking day and leftover portions on subsequent days. Key rules to enforce during generation include:

  * **Leftover timing (Rule C3):** No meal should be scheduled more than 2 days after its cooking session. This means if a recipe is cooked on Sunday, it can be served (leftover) on Monday and Tuesday, but not later. The scheduling algorithm will need to ensure a rotation such that each cooked dish is used up within 48 hours. (If a recipe has a stricter `"max_leftover_days"` attribute, that imposes an even shorter limit which must also be respected.)

  * **Cooking session composition:** Each session should ideally include dishes that complement each other in terms of preparation effort and meal coverage. A practical strategy is to include one breakfast-friendly recipe and one or two lunch/dinner recipes in each session. This aligns with the example blueprint schedule, where each cooking day prepared a breakfast (e.g. pancakes, smoothie) and a main dish (curry, soup, chili) which were then cycled as breakfasts or dinners in the following days. The system can use the `mealtime category` field (e.g. “Breakfast”, “Dinner”) to ensure each session covers appropriate meal types.

  * **Weekday breakfast convenience (Rule C4):** All breakfasts on weekdays (Monday–Friday) must either require no cooking, or be quick prep (≤15 min), or be an overnight dish. The planning algorithm will filter/ensure that any recipe slated for a weekday breakfast slot meets one of these criteria. This likely means preferring recipes tagged "Quick", "Overnight", or having CookTime = "0" for breakfasts on work days.

  * **Cost constraint (Rule C5):** No meal in the plan should exceed \$5 AUD cost per serving. Once cost\_per\_serving data is available, the system will ensure that any recipe that would break this rule is either excluded or flagged. (In practice, since high-cost recipes would have gotten a score penalty in Phase C, they are less likely to be chosen, but the rule must still be enforced strictly at scheduling time.)

  The algorithm to generate the menu can be approached as a **constraint satisfaction and optimization problem**. We have to pick \~6–9 distinct recipes (2–3 per session \* 3 sessions) out of the 100+ available, and assign them to cooking sessions and meal slots such that: all 21 meal slots (7 days \* 3 meals) are filled, no slot goes empty, leftovers are used within limits, and all nutritional goals are met or approached.

  **Proposed approach:** A hybrid of rule-based heuristics and optimization. We can start by selecting a pool of top-scoring recipes from Phase C (for example, the top 15). From these, filter or group by mealtime category (get a subset of breakfast-suitable ones and main-course ones). We then need to choose 3 breakfasts and 3–6 mains that together satisfy nutritional needs when distributed. This selection could be solved via an **integer linear programming (ILP)** or **constraint programming** solver: we could introduce binary decision variables for whether each candidate recipe is cooked in session 1, 2, or 3, and variables for whether a recipe is eaten on a particular day/meal slot. Constraints would enforce exactly 3 cook days, coverage of each day’s 3 meals, leftovers usage limits, etc. The objective could be to maximize the sum of recipe scores or to minimize deviation from nutritional targets. Using a solver like Google OR-Tools (CP-SAT) or PuLP would allow us to formally encode these constraints. However, given the complexity, a heuristic might be more practical initially:

  * Greedy construction: Pick the top-scoring dinner and breakfast recipes for the first session, schedule them for day 1 (fresh) and day 2 as leftovers. Then pick recipes for session 2 that cover the next couple of days, and so on. At each step, check constraints and adjust if something is failing (for example, if a particular nutrient is falling short, swap in a recipe high in that nutrient in a later session).

  * **Nutrient balancing:** To ensure nutrient balance over 7 days, the system will continuously calculate running totals against weekly targets as it builds the plan. For instance, if after planning 5 days it notices fiber or protein will be below target, it can choose recipes rich in those for the remaining sessions. We can use the profile’s weekly needs as a guideline – planning is essentially a small-scale optimization to meet those weekly totals while respecting daily limits. A possible strategy is to initially ignore micronutrient constraints in the selection, then evaluate in Phase E and if some constraints fail, use a backtracking or repair mechanism (e.g., swap out a recipe for another that provides more of the deficient nutrient or adjust portion distribution).

  The output of Phase D will be a **menu skeleton** JSON containing the cooking sessions (with their day and which recipes to cook) and the daily meals schedule (which recipe is eaten for breakfast, lunch, dinner each day of the week). It will also include a `nutrition_snapshot` – the aggregate daily nutrition for the plan (at least Calories, Protein, Carbs, Fat, Fiber, and daily cost). The system will compute this by summing up the per-meal nutrition for each day. This provides a quick check of how the plan meets calorie and macro goals on average.

* **Phase E – Nutrition Evaluation:** Once a draft plan is generated, the system must rigorously check it against the user’s nutritional targets and constraint rules. This phase is essentially validation of the final plan’s adequacy. The checks include:

  * Daily calories within ±10% of target (each day’s total Calories should be within 90–110% of 2100 in our example).
  * Daily fiber meeting at least the target (e.g. ≥30g fiber each day).
  * Key micronutrients (Vitamin C, Iron, Calcium, Magnesium, Zinc) at least 90% of daily targets, on average or each day (the blueprint implies daily check for those).
  * Weekly calories within ±5% of weekly target (e.g., 7-day total within 5% of 14700).
  * No breakfast violates the quick/overnight rule (already handled in construction, but double-check).
  * No meal is scheduled before its cook day and none beyond 2 days after (ensure leftover timing is correct for each entry – this catches any rule C3 or C6 violation that might have slipped through).
  * No meal with cost > \$5 (again, should be handled, but verify all `cost_per_serving` ≤ 5).

  The system will compute the nutrition totals per day from the plan: this requires summing up contributions of each recipe serving. Here, having detailed per-ingredient or per-recipe nutrient info is critical. If the ingredients DB is available, the system can calculate nutrients for each recipe serving more accurately. Otherwise, it relies on the recipe’s NutritionSummary (which may be incomplete for micronutrients). Assuming we populate the ingredient data, the tool can aggregate each day’s vitamins and minerals. Any failure in these checks will mark the plan as `"status": "fail"` in Phase E and list warnings or violations. In an automated setting, the system could then attempt adjustments (for example, if Vitamin C is low, find if any swap can be made for a recipe with higher Vitamin C, or suggest adding a fruit snack, etc.). However, the blueprint’s interactive flow would normally involve user adjustments. In our offline system, a pragmatic approach is to iterate: if the plan fails, have the algorithm automatically try a different combination or tweaks (this could tie into an optimization solver to meet constraints). Ultimately, Phase E will output a JSON indicating pass/fail and any warnings. A passed plan moves on to final output.

* **Phase F – Final Plan Output (Exports):** The system will produce the final meal plan in two JSON formats (as required for completeness). **First (Phase F1)**, an initial JSON dump containing the full plan structure including the schedule, the chosen goal and weights, a summary nutrition log (e.g., daily totals, weekly totals), and a **shopping list**. The shopping list is an aggregated list of all ingredients needed for the week’s recipes, combining like ingredients (e.g., sum of all “Oats” needed) – this can be generated by summing the ingredient quantities across all recipes in the plan. Units will remain as strings (since we might list “3 cups of Oats” etc.). This F1 output will **not** include the full recipe details for each dish, only references (like recipe names or IDs). **Next (Phase F2)**, a second JSON output will be generated that contains the `recipes_full` section: a dictionary of every recipe used in the plan, with each entry being the complete recipe object (ingredients, instructions, etc.). This essentially embeds the recipe content so the plan is self-contained. The system must ensure these recipe objects include all required fields (Category, Cuisine, Ingredients, Instructions, NutritionSummary) and are not placeholders. If any required field is missing in a recipe, that’s an error and must be caught (either remedied by adding the info from the database or aborting with an error). The final output is the two JSON structures which together represent a full 7-day meal plan package. Being offline, the system will likely just save these to files or print them for the user to use.

* **Phase G – Optional Extras:** Although not mandatory, the system could offer additional features after producing the plan. For instance, it could generate a micronutrient intake table for the week, charts of cost or nutrition distribution, or even a PDF summary of the plan. These would be implemented as separate modules that take the final plan data and create visualizations or documents. Since these must not alter the core plan JSON, they would be purely additive. In an offline environment, one could use libraries like Matplotlib or Plotly for charts, and ReportLab or WeasyPrint for PDFs. These enhancements are nice-to-have and can be part of a later phase of development.

The above design ensures that all stages A–G are executed in sequence, fully offline, and each stage’s output is well-defined. The system will be interactive only to the extent of choosing a goal (or could default to a primary goal if automation is desired), otherwise it will run autonomously to produce the meal plan.

## Modeling Strategy: Rule-Based and ML Hybrid Approach

Building this meal planning system can benefit from a combination of rule-based logic and machine learning, given the nature of the tasks:

* **Recipe Selection and Goal Matching:** The scoring method provided is essentially a weighted linear model – a *content-based recommendation* approach using domain knowledge (nutritional content and tags weighted by goal preferences). This is a rule-based formula derived from project requirements, but it could be enhanced with ML. For example, one could train a regression model or a classifier to predict how well a recipe fits a goal based on its features (nutrients, tags), possibly learning nonlinear interactions or using historical user feedback. In the absence of a large training dataset, however, the given formula is a robust starting point. A hybrid approach might use the formula to narrow down candidates, then an ML model (trained perhaps on known successful meal plans or user ratings) to fine-tune the ranking or predict which recipes the user is more likely to enjoy among the top-scoring ones. Initially, we will rely on the rule-based scoring, which is transparent and easily adjustable via the weights in `goals.json`.

* **Nutrient Balancing over 7 Days:** Achieving a balance of nutrients across the entire week is akin to solving a constrained optimization problem. A purely rule-based approach (greedy or manual heuristics) might hit or miss the perfect balance. Here, an algorithmic optimizer or an AI search method can be helpful. One approach is to use **linear programming** or **constraint solvers** as mentioned, which is more OR (operations research) than machine learning, but guarantees finding a feasible solution if one exists. Alternatively, a **genetic algorithm** or **simulated annealing** heuristic could be employed: treat each possible combination of recipes (or an entire week plan encoding) as an individual, define a fitness function that penalizes nutrient deviations and constraint violations, and then evolve a population of plans. These are metaheuristic (not exactly ML in the traditional sense, but often used in AI for optimization). They work offline and can handle complex combinations. If we wanted a true ML approach, one could formulate this as a reinforcement learning problem: an agent adds recipes to a plan one by one (or day by day), receiving rewards for meeting nutrient goals and penalties for violations, and train it via many simulated episodes to learn a policy for constructing good plans. This would require a simulation environment and careful reward shaping (e.g., a reward might be +1 for meeting all daily targets, -1 for each violation of rules, etc.). RL could eventually learn strategies to satisfy constraints, but it’s quite complex for this domain and likely overkill for now. The most pragmatic approach is a **hybrid:** use rule-based filtering and mathematical optimization to generate a balanced plan (ensuring hard constraints), and perhaps later incorporate an ML model to refine or speed up the search (for example, a model that quickly predicts if a partial plan will fail, to prune the search space).

* **Cooking Session Clustering:** Determining which recipes to cook together in one session is governed by rules (3 sessions, 2–3 recipes each, mix of meal types). This can largely be rule-based: for instance, always include 1 breakfast and 2 mains per session, or ensure that recipes sharing expensive ingredients are grouped to optimize grocery usage, etc. However, ML could assist in clustering recipes based on similarity or complementary attributes. For example, one could use k-means or hierarchical clustering on recipe feature vectors (features could include prep time, cuisine, main ingredients, etc.) to group recipes that might be efficiently cooked together (perhaps similar cuisines or techniques). Another idea is to use ML to predict the *prep synergy* of two recipes – e.g., a model could learn that “baking these two dishes together saves time” or “these two share many ingredients, good to cook in one go”. In absence of such data, simple heuristics (like grouping by category or shared ingredients) will be used. The system might also allow a slight brute-force search: try different groupings of chosen recipes into sessions and evaluate which grouping yields the best compliance with rules and convenience (minimize total prep time per session, etc.). This is manageable given only 3 sessions and a handful of recipes.

* **Tag and Goal Matching:** Tags are already handled in the scoring formula in a simplistic way (±0.5 per tag match). A more nuanced ML approach could be to train a model to predict a recipe’s suitability for a goal based on tags, perhaps using word embeddings or one-hot encodings of tags. But since the relationship is explicitly defined by the goal (in `priority_tags`), a rule-based application of those weights is straightforward. We might improve this by considering tag interactions or by learning from user feedback (e.g., if the user consistently prefers certain tags, increase their weight). For now, the rule-based use of tags is adequate. We just need to ensure tags are correctly assigned and perhaps extend the logic: e.g., *conflicting tags* might be those present in a recipe that have a negative weight in the chosen goal (already covered by subtracting 0.5 each), and *matching tags* are those present with positive weight. We should verify that all relevant tags in goals are present in recipes and vice versa (so that the tag-based scoring has the intended effect).

* **Constraint Validation and Scoring:** Hard constraints (dietary limits, cost, schedule rules) will be handled with rule-based logic – there’s little ambiguity here. The system will use simple condition checks (if-then rules) to flag any violation in Phase E. One could conceive of an ML classifier that predicts if a given plan will pass or fail constraints (which could be useful to guide a search algorithm to avoid generating failing plans), but given we can directly calculate and check these constraints, an ML model is unnecessary for validation. However, if the search space for plans is huge, one trick is to use ML to predict promising areas of that space. For example, a model could learn which nutrient combinations in chosen recipes are likely to meet daily targets. This again would require training data (i.e., many example plans labelled pass/fail), which we currently don’t have. So initially, direct computation and rule enforcement is the plan. In summary, constraint handling is rule-based and deterministic, which ensures the final output is reliable and safe.

In conclusion, the system will primarily be rule-based/algorithmic for reliability (especially given the strict must/must-not rules), while incorporating ML-inspired techniques for optimization. In early development, we lean on established methods like linear programming solvers and heuristics. As the project evolves, we can integrate machine learning to improve recommendations (for instance, personalizing recipe selection beyond the static goal weights using user feedback or using ML to optimize meal scheduling in a smarter way). This hybrid approach leverages the best of both: rules provide hard constraint satisfaction and domain knowledge, and ML can provide flexibility and learning from data when available.

## Tools, Libraries, and Data Processing Requirements

To implement the above system offline, we will utilize various Python libraries and define clear data processing steps:

* **Data Loading and Handling:** Python’s built-in `json` library (or `pandas` if converting JSON to DataFrame) will be used to read the input files. We will parse numeric values that are stored as strings (e.g., "2100" calories) into numeric types for calculations, and conversely ensure that any numeric output is formatted as a string if writing back to JSON (per the blueprint’s requirement that numeric values in JSON outputs be quoted). Basic validation of JSON schema can be done with Python (or libraries like `jsonschema` if we define a schema).

* **Numerical Computation:** Libraries like **NumPy** or **Pandas** will be useful for calculations. For example, computing recipe scores (Phase C) by vectorizing nutrient arrays and doing dot-products with the goal weight vector can be done efficiently with NumPy. Pandas can help in aggregating nutritional data by day or summing ingredients for the shopping list.

* **Optimization/Constraint Solving:** To systematically solve the menu generation problem, we might use **PuLP** (a linear programming library in Python) or **Google OR-Tools** for constraint programming. These libraries allow defining decision variables and constraints clearly and then using a solver (like CBC or SAT solvers) to find an optimal meal assignment. OR-Tools in particular is powerful for scheduling-like problems (it can handle the “no meal more than 2 days old” constraint elegantly by linking variables). If the ILP route proves too complex, we can implement a custom search using Python – possibly aided by the **itertools** library for combinations, or using **networkx** if we frame it as a flow problem (e.g., flow of recipe portions to day slots).

* **Machine Learning Libraries:** In the initial phase, heavy ML libraries (TensorFlow, PyTorch) might not be necessary. If we later incorporate an ML model (say a neural network to predict plan quality or a recommender system), **PyTorch** or **scikit-learn** could be used. Scikit-learn would suffice for simpler models (logistic regression, random forest) on tabular data (like recipes encoded by nutrients/tags). For a neural network or sequence model, PyTorch provides flexibility to design custom training loops offline. However, any ML model training would require a dataset – we can generate a synthetic dataset of meal plans or use known nutritional datasets to pre-train something (e.g., an autoencoder on nutrient profiles). This is an extension; initially the rule-based approach means we rely less on these libraries.

* **Data Pre-processing:** We will need functions to parse ingredient amounts (e.g., convert "1/2 cup" or "400g" into numeric quantities and standard units). Using a library like **`python-unit-converter`** or writing custom parsers for common units will be necessary to aggregate ingredients for the shopping list and to compute nutrition. All macro/micro values in the ingredient database will be stored as strings with units (as per blueprint), so the system needs to strip units and convert those to numbers for calculation. We’ll create a conversion map for units (teaspoons to mL, etc., if needed). For the nutritional data, if we obtain an ingredient database from an external source (like USDA FoodData), some pre-processing to map those ingredients to our recipe ingredient names will be required (ensuring “Canned Tuna” in recipes matches an entry in the DB, etc.). This likely involves some manual mapping or a fuzzy string match, which we can do offline by preparing a custom ingredients file.

* **Output Generation:** For producing the final JSON outputs, Python’s `json.dump` will be used to ensure proper JSON formatting. We’ll carefully assemble the Python dictionaries for Phase D, E, F outputs according to the required schema. No external calls or services are needed; it’s all local file writing. If we generate charts or a PDF (Phase G extras), libraries like **Matplotlib/Seaborn** (for plotting nutrient intake over days or cost distribution) and **ReportLab** or **FPDF** (for PDF reports) can be used. These would operate offline with our computed data.

* **Testing and Validation Tools:** During development, we can use unit tests to verify that each phase works correctly (e.g., a test that intentionally breaks a rule in the data to see if Phase A catches it, or a test feeding a simple set of recipes to the scheduler to see if it assigns leftovers properly). Libraries like **pytest** could be employed for automated testing of the meal planning functions. Although not part of the final system, this ensures reliability.

Overall, the technology stack is Python-centric, with a mix of data science libraries (NumPy/Pandas), possibly OR tools for optimization, and standard libraries for JSON and file I/O. Everything chosen is open-source and can be installed locally, which aligns with the offline, desktop environment requirement. Data pre-processing (especially building the ingredient nutrition/price dataset) will be a critical upfront task to empower the rest of the pipeline.

## Using Existing JSON Data for Input and Model Training

The structured JSON files provided serve as both **inputs** to the system and as a knowledge base that could be leveraged for ML training:

* **Direct Inputs:** The system directly uses `profile.json` to fetch user constraints and targets, `goals.json` to get optimization weights, and `recipes.json` (plus the ingredient data) as the search space for solutions. These files are essentially the domain data the algorithm works on. Because they are in JSON, they are easily loaded into Python objects. The consistency and completeness of these files will directly affect system performance – for example, if a certain nutrient target is missing or a recipe’s data is wrong, the plan could be suboptimal or invalid. Therefore, maintaining these as clean, updated sources is important. The JSON structure also means the system could be extended to accept new recipes or goals just by editing these files, without code changes.

* **As a Training Dataset:** If we want to introduce machine learning, the existing data can be used to derive a dataset. For instance, we have 108 recipes, each with a set of features (nutrient amounts, tags, etc.) and we could label them for each goal with a “suitability” score (using the scoring formula as a pseudo-label). This could train a model to predict recipe scores for a given goal, effectively learning to approximate the formula or even detect non-linear patterns (maybe certain combinations of nutrients matter more than the linear weights suggest). However, 108 samples may be too few for a complex model, but sufficient for a simple linear regression or decision tree to mimic the rule-based scoring. Another training use-case: if we had historical meal plans that were successful, those could serve as positive examples to train a sequence model or constraint-learning model. Without such historical plans, we could generate a number of random meal plans (or use the optimizer to produce many variations) and label them pass/fail on constraints and by total goal adherence, then train a classifier to recognize valid vs invalid plans. This classifier could then guide the search or even directly generate a plan via structured prediction. These are advanced ideas; initially the focus is on using the data directly.

* **Data Expansion:** The current structured data can be enriched to improve ML performance. For example, if we augment each recipe with additional features (micronutrient content from an external dataset), we suddenly have a richer feature set for any ML model and a more precise input for rule-based checks. We could also incorporate user preference data (if available in the future) – e.g., ratings or frequency a user likes a recipe – to train a collaborative filtering model. Although not present now, the system’s design could accommodate an additional input file (say `preferences.json`) in the future, which could then feed into a model that personalizes the plan beyond purely nutritional considerations.

* **Offline Model Serving:** If we do train any ML models (for scoring, recommendation, etc.), these models can be saved (as pickle files for sklearn models, or `.pt` for PyTorch) and loaded by the system at runtime. This allows the entire pipeline to remain offline – the model is essentially part of the local app. The structured JSON data can also serve as a lightweight database that the ML model can query (for example, a model might output “need more high-VitaminC foods” and the system can query the ingredients/recipes data for candidates).

In summary, the existing JSON data forms the backbone of the system’s knowledge. It is used directly in algorithmic computations, and it can be repurposed to create training datasets for any machine learning components we choose to add. Ensuring this data is well-structured and possibly expanding it (especially the ingredient nutrient database) will both improve the immediate rule-based system and enable future ML-driven improvements.

## Development Roadmap

To build this meal-planning system methodically, a phased development roadmap is proposed:

1. **Data Preparation Phase:**

   * Create or import the **ingredients nutrition & price database**. Ensure all ingredients from recipes have entries with standardized units, nutrition per unit, and cost. Clean up JSON formatting issues (e.g., unify keys and data types).
   * Write a **validation script** to check basic integrity of `profile.json`, `goals.json`, `recipes.json`, and the new `ingredients.json` (presence of keys, valid ranges, matching names). This corresponds to Phase A checks.

2. **Core Pipeline Implementation:**

   * Implement Phase A data loading and produce an `"ok"` or error status.
   * Implement Phase B goal selection (for now, this could be a simple function that takes a goal name and retrieves weights; later integrate a user prompt or CLI argument for goal).
   * Implement Phase C recipe scoring. Start with the exact formula provided to verify it works. Unit test this with known scenarios (e.g., if a goal prioritizes Protein highly, verify high-protein recipes get higher scores).
   * Implement a basic Phase D scheduling algorithm. Initially, you might simplify the problem: for example, pick 6 top-scoring recipes (2 per session) and try assigning them to a week pattern (Sun, Tue, Thu cook days). This can be done by greedy logic first to get a prototype working. Ensure the output format matches the expected JSON structure with `cooking_sessions` and `daily_meals`.
   * Compute a draft `nutrition_snapshot` for the plan by summing nutrients – this tests integration with the ingredient data (or NutritionSummary data).

3. **Constraint Solver & Refinement:**

   * Upgrade the Phase D algorithm by introducing constraint-solving or smarter search. For instance, incorporate checks for leftover rules as you build the schedule (don’t allow a leftover to go into a forbidden day slot). If using an ILP solver, formally add constraints and solve for an optimal plan (optimal meaning e.g. maximize total goal score or minimize nutrient deviation). Compare the solver’s output with the greedy approach for a test week to ensure it’s producing valid results.
   * Integrate **weekday breakfast rule** checking into the schedule builder (filter out recipes that don’t meet rule C4 for those slots).
   * Integrate cost considerations: ensure no recipe chosen has cost > \$5 (or if so, allow the solver to replace it).
   * By the end of this step, Phase D should reliably produce a valid schedule (or indicate if none feasible, though with a large recipe pool, a valid plan should exist).

4. **Phase E Validation & Iteration:**

   * Implement the Phase E checks exactly as specified. This will involve calculating daily totals for Calories, Fiber, etc., and weekly totals. Use the profile’s targets to verify each rule.
   * If the plan fails, implement a simple loop or adjustment mechanism. For example, if a particular nutrient is below threshold on a day, identify which meal/day is the culprit and see if swapping in a different recipe (from the pool of candidates) could fix it. This could be done with a brute-force try or using the solver with added constraints (e.g., force inclusion of a high-iron recipe if Iron was low).
   * Re-run validation after any adjustment. In an automated tool, we might allow a couple of iterations. Document any cases where the algorithm might still fail (perhaps extremely strict constraints could make it unsolvable – then the system should gracefully report inability to satisfy all conditions).

5. **Phase F Output Construction:**

   * Once Phase E passes, build the Phase F1 JSON output. This involves collating results: list the chosen goal and weights, list all chosen recipes (by name or ID) in their sessions, include the full weekly schedule, and generate the shopping list. The shopping list generation should consolidate ingredients: go through each recipe used and sum up quantities of each ingredient. This may require unit conversion if the same ingredient appears in different units (e.g., one recipe uses “1 cup of milk” and another “200ml of milk” – convert to a common unit before summing).
   * Build the Phase F2 output by pulling in the full recipe objects for each recipe in the plan from the recipe database. Ensure these objects have no placeholders and include required fields. This is straightforward as we already have them; just filter the `recipes.json` data to the ones in the plan.
   * Perform a final validation of the outputs against the blueprint’s requirements (no missing fields, correct structure). Possibly write a small JSON schema or use the blueprint’s described validation gates (Phase F2 checks) to double-check the dumps.

6. **Testing and User Acceptance:**

   * Test the entire pipeline on the provided profile and each goal scenario to see if the output makes sense (nutritionally and logically). Manually inspect a sample plan – does it have variety? Are the leftovers rotated properly? This might reveal the need for additional tuning (for example, to introduce more variety or to avoid weird meal sequences like curry for breakfast, unless intended). We may tweak the scheduling algorithm to ensure a more realistic meal distribution (perhaps enforcing that breakfasts come from the specifically breakfast-tagged recipes, etc., if not already).
   * Run edge-case tests: what if the user profile had a very high protein need? Does the system find enough high-protein recipes? What if a goal has a strong negative weight on something common (like LowEnergyFilling dislikes calories) – does the plan avoid high-calorie recipes effectively? Adjust the algorithm or data as needed.

7. **ML Enhancements (Future Phase):**

   * With the rule-based system stable, experiment with incorporating ML for improvements. For instance, implement a learning-to-rank using the recipe data: train a small model to predict the score of recipes for a given goal and compare its output to the formula (this could smooth out the scoring or allow more complex patterns).
   * Another enhancement: use a clustering or classifier to categorize recipes by cooking session suitability (maybe train on features like total cook+prep time and ingredient count to classify recipes as “quick” vs “lengthy”, then ensure sessions mix one lengthy and one quick to balance workload).
   * If user preference data is available or can be simulated, incorporate a personalization layer – e.g., a user profile could weight certain cuisines or ingredients more. This could be as simple as extra rules (avoid recipes with ingredients the user hates) or a collaborative filter if multiple users were considered.

8. **Optional Extras and UI:**

   * Develop the optional Phase G features such as generating a nutrient breakdown chart for the week or a cost chart. This can be done once the core functionality is working.
   * If the tool is to be used by end-users, consider adding a simple command-line interface or even a basic GUI to select goals and output results. This might involve packaging the solution (using PyInstaller to create an offline executable, for example).

By following this roadmap, we ensure that the project starts with solid data foundations, then implements the critical planning logic step by step, validating at each phase, and finally layering on enhancements. The end result will be a comprehensive, offline-capable meal planning system that takes the user from raw data and goals all the way to a complete weekly meal plan that is nutritionally balanced, goal-aligned, and adheres to practical cooking constraints. Throughout development, alignment with the Meal-Plan Workflow Blueprint is maintained, ensuring that the system’s outputs are not only logically sound but also formatted and validated exactly as specified in the project requirements.
